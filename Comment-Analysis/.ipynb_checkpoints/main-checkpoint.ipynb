{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff1571",
   "metadata": {},
   "source": [
    "# Download and process Youtube-Comments\n",
    "\n",
    "## Schnulzen\n",
    "Rock\n",
    "- Creep\n",
    "https://www.youtube.com/watch?v=XFkzRNyygfk\n",
    "\n",
    "Country / Acoustic Rock\n",
    "- Hurt 2002\n",
    "https://www.youtube.com/watch?v=8AHCfZTRGiI\n",
    "\n",
    "“Schlager” englisch\n",
    "- ABBA - The Winner Takes It All 1980\n",
    "https://www.youtube.com/watch?v=8tE0GjSQpes\n",
    "\n",
    "Classic / Film Musik\n",
    "- Tara's Theme ~ Gone with the Wind\n",
    "https://www.youtube.com/watch?v=PgF-rcHcPqE\n",
    "\n",
    "## Nicht-Schnulzen\n",
    "\n",
    "Rock \n",
    "- another brick in the wall 1980 - Pink Floyd\n",
    "https://www.youtube.com/watch?v=YR5ApYxkU-U\n",
    "YR5ApYxkU-U\n",
    "\n",
    "\n",
    "- ABBA - Money, Money, Money\n",
    "https://www.youtube.com/watch?v=ETxmCCsMoD0\n",
    "ETxmCCsMoD0\n",
    "\n",
    "-> Vergleich mit Abba möglich Schnulze vs nicht-schnulze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1938b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from YT_download_comments import list_from_video\n",
    "\n",
    "class video_class:\n",
    "    def __init__(self, title, artist, year, video_id, status, genre):\n",
    "        self.title = title\n",
    "        self.artist = artist\n",
    "        self.year = year\n",
    "        self.video_id = video_id\n",
    "        self.status = status\n",
    "        self.genre = genre\n",
    "        \n",
    "        self.id_name = f\"{self.artist}-{self.title}\"\n",
    "        self.download_location = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"video_class of {self.title}/{self.status}\"\n",
    "    \n",
    "    def  __repr__(self):\n",
    "        return f\"{self.title};{self.video_id}\"\n",
    "        \n",
    "    def get_comments(self, len_output, order=\"relevance\"):\n",
    "        print(f\"ca. {len_output} comments are beeing extracted in order of {order}, from {self.title}\")\n",
    "        DEVELOPER_KEY = \"AIzaSyAYvRpVKJUS5MUnw6NVcIQB484ao6CdutE\"\n",
    "        \n",
    "        \n",
    "        lst, length = list_from_video(DEVELOPER_KEY, self.video_id, order, len_output)\n",
    "        \n",
    "        print(f\"{len(lst)} comments were extracted\")        \n",
    "        return lst\n",
    "    \n",
    "'''    def reload_download_as_list(self):\n",
    "        list = []\n",
    "        with open(self.download_location, 'r') as content:\n",
    "            for i, line in enumerate(content.readlines()):\n",
    "\n",
    "                try:\n",
    "                    line = line.strip()\n",
    "\n",
    "                    #test if all columns have values\n",
    "                    content = line.split(\"\\t\")\n",
    "                    id = content[0]\n",
    "                    author = content[1]\n",
    "                    comment = content[2]\n",
    "\n",
    "                    list.append(line)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        return list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        ('Creep', 'Radiohead', 0, 'XFkzRNyygfk', 1, 'rock'),\n",
    "        ('Hurt', 'Johnny Cash', 2002, '8AHCfZTRGiI', 1, 'country;acousic rock'),\n",
    "        ('The Winner Takes It All', 'ABBA', 1980, '8tE0GjSQpes', 1, 'en-schlager'),\n",
    "        ('Taras Theme', 'Gone with the Wind', 0, 'PgF-rcHcPqE', 1, 'film-music'),\n",
    "        ('Money, Money, Money', 'ABBA', 0, 'ETxmCCsMoD0', 0, 'NA'),\n",
    "    \n",
    "        ('Thinking Out Loud', 'Ed Sheeran', 2014, 'lp-EO5I60KA', 1, 'Pop'),\n",
    "        ('Because Of You', 'Kelly Clarkson', 2005, 'Ra-Om7UMSJc', 1, 'Pop'),\n",
    "        ('All of Me', 'John Legend', 2013, '450p7goxZqg', 1, 'Pop'),\n",
    "        ('Another Love', 'Tom Odell', 2012, 'MwpMEbgC7DA', 1, 'Pop')\n",
    "        ]\n",
    "\n",
    "videos = {}\n",
    "index = 0\n",
    "for title, artist, year, video_id, status, genre in data:\n",
    "    videos[index] = video_class(title, artist, year, video_id, status, genre)\n",
    "    index += 1\n",
    "\n",
    "for i in videos.items():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst = videos[8].get_comments(len_output=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6266fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from YT_comments_filter import append_if_english, remove_emojis_string, emojies_re_pattern, english_true\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create df, save initial download\n",
    "xi = []\n",
    "for t in lst:\n",
    "    t = t + (videos[0].id_name, videos[0].status)\n",
    "    xi.append(t)\n",
    "\n",
    "df = pd.DataFrame(xi, columns =['author', 'comment', \"origin\", 'origin_status'])\n",
    "\n",
    "#save to .tsv\n",
    "filename = \"Tom-Odell-1000.tsv\"\n",
    "df.to_csv(filename, sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "# prepare comments\n",
    "\n",
    "#delete emojies\n",
    "pattern = emojies_re_pattern()\n",
    "df['comment'] = df['comment'].apply(lambda x: pattern.sub('', str(x)))\n",
    "\n",
    "#delete newlines\n",
    "#pattern = re.compile(\"\\n\", re.UNICODE)\n",
    "#df['comment'] = df['comment'].apply(lambda x: pattern.sub(' ', str(x)))\n",
    "\n",
    "#tidy\n",
    "pattern = re.compile(\"  \", re.UNICODE)\n",
    "df['comment'] = df['comment'].apply(lambda x: pattern.sub(' ', str(x)))\n",
    "\n",
    "\n",
    "\n",
    "#delete if non-english\n",
    "            \n",
    "percent = 0.40\n",
    "print(10*\"*\")\n",
    "print(f\"delete if not more than {percent*100}% english words\")\n",
    "\n",
    "#df['comment'] = df['comment'].apply(lambda x : x if english_true(x, percent) == True else \"NA\" )\n",
    "df['english'] = df['comment'].apply(lambda x : english_true(x, percent))\n",
    "\n",
    "\n",
    "# drop NA\n",
    "df['comment'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "\n",
    "print(f\"comments: {len(lst)-1}\")\n",
    "\n",
    "\n",
    "print(df)\n",
    "filename = \"Tom-Odell-1000-PP.tsv\"\n",
    "df.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5af34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Ed-Sheeran-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df2 = pd.read_csv(\"John-Legend-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df3 = pd.read_csv(\"Kelly-Clarkson-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df4 = pd.read_csv(\"Tom-Odell-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "\n",
    "df1 = df1[df1[\"english\"] == True]\n",
    "print(len(df1))\n",
    "df2 = df2[df2[\"english\"] == True]\n",
    "print(len(df2))\n",
    "df3 = df3[df3[\"english\"] == True]\n",
    "print(len(df3))\n",
    "df4 = df4[df4[\"english\"] == True]\n",
    "print(len(df4))\n",
    "\n",
    "df1 = df1.head(250)\n",
    "df2 = df2.head(250)\n",
    "df3 = df3.head(250)\n",
    "df4 = df4.head(250)\n",
    "\n",
    "df_final = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "print(df_final)\n",
    "filename = \"final_annotation.tsv\"\n",
    "df_final.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4282b",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7870af",
   "metadata": {},
   "source": [
    "## Import annotated Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0559baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['author', 'comment', 'origin', 'origin_status', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"annotate_1000_final.tsv\"\n",
    "annotations = pd.read_csv(filename, sep=\"\\t\")\n",
    "selected_columns = [\"author\", \"comment\", \"origin\", \"origin_status\", \"category\"]\n",
    "annotations = annotations[selected_columns]\n",
    "print(annotations.columns)\n",
    "\n",
    "df_comments_s = annotations[annotations[\"category\"] == 1]\n",
    "df_comments_ns = annotations[annotations[\"category\"] == 0]\n",
    "\n",
    "txt_s = combined_string = \" \".join(df_comments_s[\"comment\"].astype(str))\n",
    "txt_ns = combined_string = \" \".join(df_comments_ns[\"comment\"].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a10df3",
   "metadata": {},
   "source": [
    "## exploration of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Exploration\n",
    "from YT_comments_filter import load_tsv_list, comments_inone_string\n",
    "from simple_ml import get_keywords_from_string\n",
    "\n",
    "doc = txt_s\n",
    "keywords = get_keywords_from_string(doc, word_range=1, top_n=20, stop_words = \"english\")\n",
    "for word in keywords:\n",
    "    print(word)\n",
    "\n",
    "print(10*\"*\")    \n",
    "\n",
    "doc = txt_ns\n",
    "keywords = get_keywords_from_string(doc, word_range=1, top_n=20, stop_words = \"english\")\n",
    "for word in keywords:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b943",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "### Create Train- and Evaluation-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133c8788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# https://pypi.org/project/simpletransformers/ <- source for logging and evaluation tipps\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from sys import stderr\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# toggle logging\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "\n",
    "## prepare annotations\n",
    "#delete non-category\n",
    "annotations = annotations[annotations[\"category\"] != -1]\n",
    "\n",
    "#shuffle data and drop old index\n",
    "annotations = annotations.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# create training-data <-- df\n",
    "train_df = annotations.head(int(len(annotations)*0.8))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "train_df = train_df[selected_columns]\n",
    "\n",
    "# create eval-data <-- df\n",
    "eval_df = annotations.tail(int(len(annotations)*0.2))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "eval_df = eval_df[selected_columns]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(eval_df))\n",
    "\n",
    "\n",
    "### Use Model\n",
    "#apply_data = []\n",
    "#with open('sample_data/apply.tsv', \"r\") as file_content:\n",
    "#    for line in file_content.readlines():\n",
    "#        line = line.strip()\n",
    "#        apply_data.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980ca74",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62cd121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/qbukold/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  0%|                                           | 2/752 [00:05<32:35,  2.61s/it]\n",
      "Epoch 1 of 1:   0%|                                       | 0/1 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 1:   0%|                              | 0/94 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6579:   0%|               | 0/94 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6579:   1%|       | 1/94 [00:01<02:23,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7050:   1%|       | 1/94 [00:01<02:23,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7050:   2%|▏      | 2/94 [00:03<02:23,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6816:   2%|▏      | 2/94 [00:03<02:23,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6816:   3%|▏      | 3/94 [00:04<02:18,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6669:   3%|▏      | 3/94 [00:04<02:18,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6669:   4%|▎      | 4/94 [00:06<02:15,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7082:   4%|▎      | 4/94 [00:06<02:15,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7082:   5%|▎      | 5/94 [00:07<02:12,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6424:   5%|▎      | 5/94 [00:07<02:12,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6424:   6%|▍      | 6/94 [00:09<02:12,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5994:   6%|▍      | 6/94 [00:09<02:12,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5994:   7%|▌      | 7/94 [00:10<02:11,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7107:   7%|▌      | 7/94 [00:10<02:11,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7107:   9%|▌      | 8/94 [00:11<02:06,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6527:   9%|▌      | 8/94 [00:12<02:06,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6527:  10%|▋      | 9/94 [00:13<02:05,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6242:  10%|▋      | 9/94 [00:13<02:05,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6242:  11%|▋     | 10/94 [00:15<02:16,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8897:  11%|▋     | 10/94 [00:15<02:16,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8897:  12%|▋     | 11/94 [00:17<02:22,  1.71s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5200:  12%|▋     | 11/94 [00:17<02:22,  1.71s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5200:  13%|▊     | 12/94 [00:19<02:23,  1.75s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5101:  13%|▊     | 12/94 [00:19<02:23,  1.75s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5101:  14%|▊     | 13/94 [00:21<02:24,  1.78s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5636:  14%|▊     | 13/94 [00:21<02:24,  1.78s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5636:  15%|▉     | 14/94 [00:22<02:17,  1.72s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4834:  15%|▉     | 14/94 [00:22<02:17,  1.72s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4834:  16%|▉     | 15/94 [00:23<02:06,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5267:  16%|▉     | 15/94 [00:24<02:06,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5267:  17%|█     | 16/94 [00:25<01:59,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4390:  17%|█     | 16/94 [00:25<01:59,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4390:  18%|█     | 17/94 [00:26<01:55,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5890:  18%|█     | 17/94 [00:27<01:55,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5890:  19%|█▏    | 18/94 [00:28<01:55,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.9246:  19%|█▏    | 18/94 [00:28<01:55,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.9246:  20%|█▏    | 19/94 [00:29<01:55,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2184:  20%|█▏    | 19/94 [00:30<01:55,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2184:  21%|█▎    | 20/94 [00:31<01:52,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6651:  21%|█▎    | 20/94 [00:31<01:52,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6651:  22%|█▎    | 21/94 [00:32<01:53,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7190:  22%|█▎    | 21/94 [00:33<01:53,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7190:  23%|█▍    | 22/94 [00:34<01:50,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6544:  23%|█▍    | 22/94 [00:34<01:50,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6544:  24%|█▍    | 23/94 [00:35<01:46,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6158:  24%|█▍    | 23/94 [00:36<01:46,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6158:  26%|█▌    | 24/94 [00:37<01:47,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3343:  26%|█▌    | 24/94 [00:37<01:47,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3343:  27%|█▌    | 25/94 [00:39<01:45,  1.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0/1. Running Loss:    0.6019:  27%|█▌    | 25/94 [00:39<01:45,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6019:  28%|█▋    | 26/94 [00:40<01:44,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5374:  28%|█▋    | 26/94 [00:41<01:44,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5374:  29%|█▋    | 27/94 [00:42<01:53,  1.70s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4794:  29%|█▋    | 27/94 [00:43<01:53,  1.70s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4794:  30%|█▊    | 28/94 [00:44<01:54,  1.74s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5137:  30%|█▊    | 28/94 [00:44<01:54,  1.74s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5137:  31%|█▊    | 29/94 [00:46<01:49,  1.68s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6691:  31%|█▊    | 29/94 [00:46<01:49,  1.68s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6691:  32%|█▉    | 30/94 [00:47<01:46,  1.67s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4009:  32%|█▉    | 30/94 [00:48<01:46,  1.67s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4009:  33%|█▉    | 31/94 [00:49<01:39,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4554:  33%|█▉    | 31/94 [00:49<01:39,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4554:  34%|██    | 32/94 [00:50<01:33,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4622:  34%|██    | 32/94 [00:50<01:33,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4622:  35%|██    | 33/94 [00:51<01:29,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4265:  35%|██    | 33/94 [00:52<01:29,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4265:  36%|██▏   | 34/94 [00:53<01:25,  1.43s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3878:  36%|██▏   | 34/94 [00:53<01:25,  1.43s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3878:  37%|██▏   | 35/94 [00:54<01:22,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4180:  37%|██▏   | 35/94 [00:54<01:22,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4180:  38%|██▎   | 36/94 [00:55<01:20,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6166:  38%|██▎   | 36/94 [00:56<01:20,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6166:  39%|██▎   | 37/94 [00:57<01:22,  1.45s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6888:  39%|██▎   | 37/94 [00:58<01:22,  1.45s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6888:  40%|██▍   | 38/94 [00:59<01:31,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8040:  40%|██▍   | 38/94 [01:00<01:31,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8040:  41%|██▍   | 39/94 [01:01<01:34,  1.73s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5701:  41%|██▍   | 39/94 [01:01<01:34,  1.73s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5701:  43%|██▌   | 40/94 [01:03<01:32,  1.71s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5340:  43%|██▌   | 40/94 [01:03<01:32,  1.71s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5340:  44%|██▌   | 41/94 [01:04<01:28,  1.67s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2918:  44%|██▌   | 41/94 [01:05<01:28,  1.67s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2918:  45%|██▋   | 42/94 [01:06<01:25,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2989:  45%|██▋   | 42/94 [01:06<01:25,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2989:  46%|██▋   | 43/94 [01:07<01:23,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2152:  46%|██▋   | 43/94 [01:08<01:23,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2152:  47%|██▊   | 44/94 [01:09<01:23,  1.68s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1463:  47%|██▊   | 44/94 [01:09<01:23,  1.68s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1463:  48%|██▊   | 45/94 [01:11<01:18,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2736:  48%|██▊   | 45/94 [01:11<01:18,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2736:  49%|██▉   | 46/94 [01:12<01:14,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2089:  49%|██▉   | 46/94 [01:12<01:14,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2089:  50%|███   | 47/94 [01:13<01:11,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2791:  50%|███   | 47/94 [01:14<01:11,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2791:  51%|███   | 48/94 [01:15<01:08,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4765:  51%|███   | 48/94 [01:15<01:08,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4765:  52%|███▏  | 49/94 [01:16<01:07,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3232:  52%|███▏  | 49/94 [01:17<01:07,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3232:  53%|███▏  | 50/94 [01:18<01:06,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6572:  53%|███▏  | 50/94 [01:18<01:06,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6572:  54%|███▎  | 51/94 [01:19<01:04,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5014:  54%|███▎  | 51/94 [01:20<01:04,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5014:  55%|███▎  | 52/94 [01:21<01:02,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1494:  55%|███▎  | 52/94 [01:21<01:02,  1.49s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1494:  56%|███▍  | 53/94 [01:22<01:01,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5041:  56%|███▍  | 53/94 [01:23<01:01,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5041:  57%|███▍  | 54/94 [01:24<01:02,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3239:  57%|███▍  | 54/94 [01:25<01:02,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3239:  59%|███▌  | 55/94 [01:26<01:01,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1388:  59%|███▌  | 55/94 [01:26<01:01,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1388:  60%|███▌  | 56/94 [01:27<01:01,  1.62s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2303:  60%|███▌  | 56/94 [01:28<01:01,  1.62s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2303:  61%|███▋  | 57/94 [01:29<00:59,  1.60s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1268:  61%|███▋  | 57/94 [01:29<00:59,  1.60s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1268:  62%|███▋  | 58/94 [01:30<00:55,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8284:  62%|███▋  | 58/94 [01:31<00:55,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8284:  63%|███▊  | 59/94 [01:32<00:53,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5080:  63%|███▊  | 59/94 [01:32<00:53,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5080:  64%|███▊  | 60/94 [01:33<00:52,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3577:  64%|███▊  | 60/94 [01:34<00:52,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3577:  65%|███▉  | 61/94 [01:35<00:50,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1771:  65%|███▉  | 61/94 [01:35<00:50,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1771:  66%|███▉  | 62/94 [01:36<00:48,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2470:  66%|███▉  | 62/94 [01:37<00:48,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2470:  67%|████  | 63/94 [01:38<00:46,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2515:  67%|████  | 63/94 [01:38<00:46,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2515:  68%|████  | 64/94 [01:40<00:46,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2764:  68%|████  | 64/94 [01:40<00:46,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2764:  69%|████▏ | 65/94 [01:41<00:46,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4305:  69%|████▏ | 65/94 [01:42<00:46,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4305:  70%|████▏ | 66/94 [01:43<00:44,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3211:  70%|████▏ | 66/94 [01:43<00:44,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3211:  71%|████▎ | 67/94 [01:44<00:41,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4558:  71%|████▎ | 67/94 [01:45<00:41,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4558:  72%|████▎ | 68/94 [01:46<00:39,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1470:  72%|████▎ | 68/94 [01:46<00:39,  1.52s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1470:  73%|████▍ | 69/94 [01:47<00:37,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2044:  73%|████▍ | 69/94 [01:48<00:37,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2044:  74%|████▍ | 70/94 [01:49<00:35,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4494:  74%|████▍ | 70/94 [01:49<00:35,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4494:  76%|████▌ | 71/94 [01:50<00:34,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3218:  76%|████▌ | 71/94 [01:50<00:34,  1.50s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3218:  77%|████▌ | 72/94 [01:52<00:32,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5783:  77%|████▌ | 72/94 [01:52<00:32,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5783:  78%|████▋ | 73/94 [01:53<00:30,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4107:  78%|████▋ | 73/94 [01:53<00:30,  1.47s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0/1. Running Loss:    0.4107:  79%|████▋ | 74/94 [01:54<00:29,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4494:  79%|████▋ | 74/94 [01:55<00:29,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4494:  80%|████▊ | 75/94 [01:56<00:28,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1885:  80%|████▊ | 75/94 [01:57<00:28,  1.51s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1885:  81%|████▊ | 76/94 [01:58<00:28,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4533:  81%|████▊ | 76/94 [01:58<00:28,  1.59s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4533:  82%|████▉ | 77/94 [01:59<00:26,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6568:  82%|████▉ | 77/94 [02:00<00:26,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6568:  83%|████▉ | 78/94 [02:01<00:24,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1139:  83%|████▉ | 78/94 [02:01<00:24,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1139:  84%|█████ | 79/94 [02:02<00:23,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5602:  84%|█████ | 79/94 [02:03<00:23,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5602:  85%|█████ | 80/94 [02:04<00:21,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.0914:  85%|█████ | 80/94 [02:04<00:21,  1.55s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.0914:  86%|█████▏| 81/94 [02:06<00:20,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3474:  86%|█████▏| 81/94 [02:06<00:20,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3474:  87%|█████▏| 82/94 [02:07<00:18,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3300:  87%|█████▏| 82/94 [02:08<00:18,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3300:  88%|█████▎| 83/94 [02:09<00:17,  1.57s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3194:  88%|█████▎| 83/94 [02:09<00:17,  1.57s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3194:  89%|█████▎| 84/94 [02:10<00:15,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1090:  89%|█████▎| 84/94 [02:11<00:15,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1090:  90%|█████▍| 85/94 [02:12<00:14,  1.62s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8007:  90%|█████▍| 85/94 [02:12<00:14,  1.62s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8007:  91%|█████▍| 86/94 [02:14<00:13,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6689:  91%|█████▍| 86/94 [02:14<00:13,  1.64s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6689:  93%|█████▌| 87/94 [02:15<00:11,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6754:  93%|█████▌| 87/94 [02:16<00:11,  1.63s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6754:  94%|█████▌| 88/94 [02:17<00:09,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1047:  94%|█████▌| 88/94 [02:17<00:09,  1.61s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1047:  95%|█████▋| 89/94 [02:18<00:07,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2971:  95%|█████▋| 89/94 [02:19<00:07,  1.58s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2971:  96%|█████▋| 90/94 [02:20<00:06,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6149:  96%|█████▋| 90/94 [02:20<00:06,  1.54s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6149:  97%|█████▊| 91/94 [02:21<00:04,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3582:  97%|█████▊| 91/94 [02:22<00:04,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3582:  98%|█████▊| 92/94 [02:23<00:03,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4829:  98%|█████▊| 92/94 [02:23<00:03,  1.53s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4829:  99%|█████▉| 93/94 [02:24<00:01,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4798:  99%|█████▉| 93/94 [02:25<00:01,  1.56s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4798: 100%|██████| 94/94 [02:26<00:00,  1.56s/it]\u001b[A\n",
      "Configuration saved in outputs/checkpoint-94-epoch-1/config.json\n",
      "Model weights saved in outputs/checkpoint-94-epoch-1/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-94-epoch-1/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-94-epoch-1/special_tokens_map.json\n",
      "Epoch 1 of 1: 100%|██████████████████████████████| 1/1 [02:27<00:00, 147.35s/it]\n",
      "Configuration saved in outputs/config.json\n",
      "Model weights saved in outputs/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/tokenizer_config.json\n",
      "Special tokens file saved in outputs/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94, 0.4583048119031368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TOKENIZERS_PARALLELISM=True\n",
    "import torch\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir= True)\n",
    "\n",
    "model = ClassificationModel('distilbert', 'distilbert-base-uncased', use_cuda=False,\n",
    "                            args=model_args)\n",
    "\n",
    "# start training\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66301a7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8f5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  1%|▏                                          | 1/188 [00:04<15:16,  4.90s/it]\n",
      "Running Evaluation: 100%|███████████████████████| 24/24 [00:06<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "mcc 0.48565297750505265\n",
      "tp 26\n",
      "tn 127\n",
      "fp 12\n",
      "fn 23\n",
      "auroc 0.8637498164733519\n",
      "auprc 0.7093335722698967\n",
      "eval_loss 0.40485283856590587\n",
      "Precision:\n",
      "0.8138297872340425\n",
      "35\n",
      "Annotation Label: 1\n",
      "When you are happy you enjoy the song and when you’re sad you understand the lyrics\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "If you have ever been in love, you have got to be loving this love song. Beautiful.\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "Lyrics ....\n",
      "\n",
      "When your legs don't work like they used to before\n",
      "And I can't sweep you off of your feet\n",
      "Will your mouth still remember the taste of my love\n",
      "Will your eyes still smile from your cheeks\n",
      "And darling I will be loving you 'til we're 70\n",
      "And baby my heart could still fall as hard at 23\n",
      "And I'm thinking 'bout how people fall in love in mysterious ways\n",
      "Maybe just the touch of a hand\n",
      "Oh me I fall in love with you every single day\n",
      "And I just wanna tell you I am\n",
      "So honey now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "Maybe we found love right where we are\n",
      "When my hair's all but gone and my memory fades\n",
      "And the crowds don't remember my name\n",
      "When my hands don't play the strings the same way, mm\n",
      "I know you will still love me the same\n",
      "'Cause honey your soul can never grow old, it's evergreen\n",
      "Baby your smile's forever in my mind and memory\n",
      "I'm thinking 'bout how people fall in love in mysterious ways\n",
      "Maybe it's all part of a plan\n",
      "I'll just keep on making the same mistakes\n",
      "Hoping that you'll understand\n",
      "But baby now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "That maybe we found love right where we are, oh\n",
      "So baby now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Oh darling, place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "That maybe we found love right where we are\n",
      "Oh baby, we found love right where we are (maybe)\n",
      "And we found love right where we are\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "First-time l listened to the song without depression a few months ago but today I was listening to the song with severe depression and now I know the full meaning of the song\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Perfect song for wedding \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "can we talk about the music video for a sec?\n",
      "how he sits there and looks straight into the camera, while the woman literally destroys the room. I interpret it that way, that Tom is so distracted and dissociated because of his past experience and pain, that he cannot worship his new love. he wants to, he also sings, that he'll protect her and if anyone says somethig bad he'll use his voice and \"be so fucking rude\". but yet he cannot give her what he gave his past love. so he lets her destroy everything. he maybe wants to stop it, but can't.\n",
      "\n",
      "the woman on the other hand, wants to try to reach him. she wants be there for him and listen. more specifically, she wants to love and be loved. eventually she loses her patience and screams at him. she starts to destroy things out of rage and hurt. because she loves him, but she feels like he doesn't love her. like he doesn't listen or care about her. they're both hurt. no matter what she says or does, Tom just doesn't answer her. \n",
      "He looks so numb, she gives up and leaves.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Wow, she dances so beautifully, I wish I would dance like that with someone \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "17 years deep into a marriage with two teenagers. My love for my wife is depicted here. Still!\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I can’t believe this song is 7 years old feels like yesterday.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "*THIS SONG REMİS ME FROM MY PAST DAYS OR A PERFECT SONG I CAN'T EXPRESS MYSELF SO TİME FLOWS SO FAST...*\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "this song makes you cherish the one you love and always wonder on the magic of meeting each other. keep the love alive!\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I Dying of love for Ed Sheeran\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "2013: Heard this song for the first time. \r\n",
      "2014: Showed my girlfriend this song months later. \r\n",
      "2015: many months later we both fell in love with this song. \r\n",
      "2016: Went to a JL concert with my girlfriend. \r\n",
      "2017: Sang this song and proposed to my girlfriend. \r\n",
      "2018: Got married and danced to this song at our wedding. \r\n",
      "2019: Still in love with this song. \r\n",
      "2020: so we're still together. And we're trying to start a family, But we're worried about when all this COVID-19 pandemic will end. \r\n",
      "2021: My wife got pregnant everyone!!\r\n",
      "2022: she’s due in July, I’m gonna be a dad!!!\r\n",
      "2023 i am a dad!!!(Thanks for all of your guys love)\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "If this song doesn't play at my wedding then I'm not coming.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "RIP Christine. As long as this song makes me cry you will never be forgotten my sista \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I don't know but when the first time I heard this song my heart and mind says it's for my father who broke me\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I couldn't help myself crying \n",
      "Literally I can't stop \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I dedicate this song to someone who nearly broke me completely. Thank you God for having other plans for me. Thank you Kelly for this song.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Such a beautiful song. I dedicate this song to my fiance.\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I remember singing at the top of my lungs with my eyes balling when I was a little girl because I grew up with a narcissist as my parent and I related to this song so heavily… now I hear it for the first time in ages and I realized how much I’ve grown and healed. And listened to it without a single tear! I’ve fought for my respect after learning it was demanded of me while I never had any to begin with and defended others who were victims to mental, verbal, and physical abuse. I feel incredibly proud of myself, I didn’t realize how strong I became until now. Thank you, Kelly. \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "She wrote this when she was 16. Very powerful song to be written at any age, let alone as a teenager. I think sometimes we forget just how good a writer Kelly is. Sometimes her songwriting gets over shadowed by her unbelievably beautiful voice. We all have our own opinions about music, but for me Kelly is the best singer I've ever heard.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "August 2019. A lot of memories are back\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "It is really wonderful and deserves the title of Golden Throat. It really became my favorite song, and thank you for this song \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "2012: cry\n",
      "2020: still crying\n",
      "Edit : 2021 still cry\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "The story behind this song is amazing . One of Ed's closest friend Amy Wadge was running out of cash and she was unable to pay her mortgage . Ed let her co-write Thinking Out Loud and he paid for her task . This song changed the life of Ed and Amy . Ed man of words\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "It's not just a song, it's a feeling...\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "Reasons why you should stay alive.\n",
      "\n",
      "\n",
      "\n",
      "1. We would miss you. \n",
      "2. It's not worth the regret. Either by yourself if you failed or just simply left scars, or the regret everyone else feels by not doing enough to help you. \n",
      "3. It does get better. Believe it or not it will eventually get better. Sometimes you have to go through the storm to get to the rainbow. \n",
      "4. There's so much you would miss out on doing. \n",
      "5. There is always a reason to live. It might not be clear right now, but it is always there. \n",
      "6. So many people care, and it would hurt them if you hurt yourself. \n",
      "7. You ARE worth it. Don't let anyone, especially yourself, tell you otherwise. \n",
      "8. You are amazing. \n",
      "9. A time will come, once you've battled the toughest times of your life and are in ease once again, where you will be so glad that you decided to keep on living. You will emerge stronger from this all, and won't regret your choice to carry on with life. Because things always get better. \n",
      "10. What about all the things you've always wanted to do? What about the things you've planned, but never got around to doing? You can't do them when you're dead. \n",
      "11. I love you. Even if only one person loves you, that's still a reason to stay alive. \n",
      "12. You won't be able to listen to music if you die. \n",
      "13. Killing yourself is never worth it. You'll hurt both yourself and all the people you care about. \n",
      "14. There are so many people that would miss you, including me. \n",
      "15. You're preventing a future generation, YOUR KIDS, from even being born. \n",
      "16. How do you think your family would feel? Would it improve their lives if you died? \n",
      "17. You're gorgeous, amazing, and to someone you are perfect. \n",
      "18. Think about your favourite music artist, you'll never hear their voice again... \n",
      "19. You'll never have the feeling of walking into a warm building on a cold day \n",
      "20. Listening to incredibly loud music \n",
      "21. Being alive is just really good. \n",
      "22. Not being alive is really bad. \n",
      "23. Finding your soulmate. \n",
      "24. Red pandas \n",
      "25. Going to diners at three in the morning. \n",
      "26. Really soft pillows. \n",
      "27. Eating pizza in New York City. \n",
      "28. Proving people wrong with your success. \n",
      "29. Watching the jerks that doubted you fail at life. \n",
      "30. Seeing someone trip over a garbage can. \n",
      "31. Being able to help other people. \n",
      "32. Bonfires. \n",
      "33. Sitting on rooftops. \n",
      "34. Seeing every single country in the world. \n",
      "35. Going on roadtrips. \n",
      "36. You might win the lottery someday. \n",
      "37. Listening to music on a record player. \n",
      "38. Going to the top of the Eiffel Tower. \n",
      "39. Taking really cool pictures. \n",
      "40. Literally meeting thousands of new people. \n",
      "41. Hearing crazy stories. \n",
      "42. Telling crazy stories. \n",
      "43. Eating ice cream on a hot day. \n",
      "44. More Harry Potter books could come out, you never know. \n",
      "45. Travelling to another planet someday. \n",
      "46. Having an underwater house. \n",
      "47. Randomly running into your hero on the street. \n",
      "48. Having your own room at a fancy hotel. \n",
      "49. Trampolines. \n",
      "50. Think about your favourite movie, you'll never watch it again. \n",
      "51. Think about the feeling of laughing out loud in a public place because your best friend has just sent you an inside joke, \n",
      "52. Your survival will make the world better, even if it's for just one person or 20 or 100 or more. \n",
      "53. People do care. \n",
      "54. Treehouses \n",
      "55. Hanging out with your soul mate in a treehouse \n",
      "55. Snorting when you laugh and not caring who sees \n",
      "56. I don't even know you and I love you. \n",
      "57. I don't even know you and I care about you. \n",
      "58. Because nobody is going to be like you ever, so embrace your uniqueness! \n",
      "59. You won't be here to experience the first cat world emperor. \n",
      "60. WHAT ABOUT FOOD?! YOU'LL MISS CHOCOLATE AND ALL THE OTHER NOM THINGS! \n",
      "61. Starbucks. \n",
      "62. Hugs. \n",
      "63. Stargazing. \n",
      "64. You have a purpose, and it's up to you to find out what it is. \n",
      "65. You've changed somebody's life. \n",
      "66. Now you could change the world. \n",
      "67. You will meet the person that's perfect for you. \n",
      "68. No matter how much or how little, you have your life ahead of you. \n",
      "69. You have the chance to save somebody's life. \n",
      "70. If you end your life, you're stopping yourself from achieving great things. \n",
      "71. Making snow angels. \n",
      "72. Making snowmen. \n",
      "73. Snowball fights. \n",
      "74. Life is what you make of it. \n",
      "75. Everybody has a talent. \n",
      "76. Laughing until you cry. \n",
      "77. Having the ability to be sad means you have the ability to be happy. \n",
      "78. The world would not be the same if you didn't exist. \n",
      "79. Its possible to turn frowns, upside down \n",
      "80. Be yourself, don't take anyone's shit, and never let them take you alive. \n",
      "81. Heroes are ordinary people who make themselves extraordinary. Be your own hero. \n",
      "82. Being happy doesn't mean that everything is perfect. It means that you've decided to look beyond the imperfections. \n",
      "83. One day your smile will be real. \n",
      "84. Having a really hot, relaxing bath after a stressful day. \n",
      "85. Lying on grass and laughing at the clouds. \n",
      "86. Getting completely smashed with your best friends. \n",
      "87. Eating crazy food. \n",
      "88. Staying up all night watching your favourite films with a loved one. \n",
      "89. Sleeping in all day. \n",
      "90. Creating something you're proud of. \n",
      "91. You can look back on yourself 70 years later and being proud you didn't commit \n",
      "92. Being able to meet your Internet friends. \n",
      "93. Tea / Coffee / Hot Chocolate \n",
      "94. The new season of Sherlock\n",
      "95. Cuddling under the stars. \n",
      "96. Being stupid in public because you just can. \n",
      "97. If you are reading this then you are alive! Is there any more reason to smile? \n",
      "98. being able to hug that one person you havent seen in years \n",
      "99. People care enough about you and your future to come up with 100 reasons for you not to do this. \n",
      "100. But, the final and most important one is, just, being able to experience life. Because even if your life doesn't seem so great right now, literally anything could happen\n",
      "-\n",
      "\n",
      "From someone that cares about you <3\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Call me a hopeless romantic, for I'll agree, this is such a touching, romantic song! What woman wouldn't want their true love to sing them this song!!!\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "This song resonated with me before I even truly realized the extent of the abuse I was actually putting up with as a child\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "this will be the song I dance to at my wedding\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "This helped me through every break up in my life.\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "It’s sad when your parents don’t want each other anymore. It’s even sadder when they don’t want you, either\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "“My heart can’t possibly break when it wasn’t even whole to start with” so profound !\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Once again listening this year as well during valentine week thinking about my Crush \n",
      "Don't you guys think about your crush as well?! \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "You know a song is legendary when after almost 10 years it still makes you cry\n",
      "++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### start eval\n",
    "print(\"Starting Evaluation:\")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "print(\"Results:\")\n",
    "for i in result:\n",
    "    print(i, result[i])\n",
    "\n",
    "#tp = true positives\n",
    "#tn = true negatives\n",
    "#fp = false positives\n",
    "#fn = false negaives\n",
    "#auroc = precision\n",
    "#auprc = average precison\n",
    "    \n",
    "print(\"Precision:\")\n",
    "print((len(eval_df)-len(wrong_predictions))/len(eval_df))\n",
    "    \n",
    "print(len(wrong_predictions))\n",
    "\n",
    "for n, dic in enumerate(wrong_predictions):\n",
    "    print(\"Annotation Label:\", dic.label)\n",
    "    print(dic.text_a)\n",
    "    print(10*'+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f9e84",
   "metadata": {},
   "source": [
    "## Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bca461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:05<08:32,  5.18s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:03<00:00,  3.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# create test-data <-- List\n",
    "filename = \"soph_q/test.tsv\"\n",
    "apply_df = pd.read_csv(filename, sep=\"\\t\")\n",
    "apply_lst = apply_df['comment'].tolist()\n",
    "apply_lst = apply_lst[-100:]\n",
    "print(len(apply_lst))\n",
    "\n",
    "predictions, raw_outputs = model.predict(apply_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936dcb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results saved in output.tsv\n"
     ]
    }
   ],
   "source": [
    "### output predictions\n",
    "import re\n",
    "final = []\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "  comment_result = (apply_lst[i], str(prediction), str(raw_outputs[i]))\n",
    "  final.append(comment_result)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(final, columns =['txt', 'prediction', 'probablility'])\n",
    "\n",
    "filename = 'output.tsv'\n",
    "df.to_csv(filename, sep='\\t')\n",
    "print(f\"results saved in {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc4b53",
   "metadata": {},
   "source": [
    "## Visualise Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_ml_env2)",
   "language": "python",
   "name": "my_ml_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
